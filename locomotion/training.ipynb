{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from brax import envs\n",
    "import functools\n",
    "from brax.training.agents.ppo import networks as ppo_networks\n",
    "\n",
    "envs.register_environment('barkour', BarkourEnv)\n",
    "\n",
    "env_name = 'barkour'\n",
    "env = envs.get_environment(env_name)\n",
    "\n",
    "ckpt_path = epath.Path('/tmp/quadrupred_joystick/ckpts')\n",
    "ckpt_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def policy_params_fn(current_step, make_policy, params):\n",
    "  # save checkpoints\n",
    "  orbax_checkpointer = ocp.PyTreeCheckpointer()\n",
    "  save_args = orbax_utils.save_args_from_target(params)\n",
    "  path = ckpt_path / f'{current_step}'\n",
    "  orbax_checkpointer.save(path, params, force=True, save_args=save_args)\n",
    "\n",
    "\n",
    "make_networks_factory = functools.partial(\n",
    "    ppo_networks.make_ppo_networks,\n",
    "        policy_hidden_layer_sizes=(128, 128, 128, 128))\n",
    "train_fn = functools.partial(\n",
    "      ppo.train, num_timesteps=100_000_000, num_evals=10,\n",
    "      reward_scaling=1, episode_length=1000, normalize_observations=True,\n",
    "      action_repeat=1, unroll_length=20, num_minibatches=32,\n",
    "      num_updates_per_batch=4, discounting=0.97, learning_rate=3.0e-4,\n",
    "      entropy_cost=1e-2, num_envs=8192, batch_size=256,\n",
    "      network_factory=make_networks_factory,\n",
    "      randomization_fn=domain_randomize,\n",
    "      policy_params_fn=policy_params_fn,\n",
    "      seed=0)\n",
    "\n",
    "x_data = []\n",
    "y_data = []\n",
    "ydataerr = []\n",
    "times = [datetime.now()]\n",
    "max_y, min_y = 40, 0\n",
    "\n",
    "# Reset environments since internals may be overwritten by tracers from the\n",
    "# domain randomization function.\n",
    "env = envs.get_environment(env_name)\n",
    "eval_env = envs.get_environment(env_name)\n",
    "make_inference_fn, params, _= train_fn(environment=env,\n",
    "                                       progress_fn=progress,\n",
    "                                       eval_env=eval_env)\n",
    "\n",
    "print(f'time to jit: {times[1] - times[0]}')\n",
    "print(f'time to train: {times[-1] - times[1]}')\n",
    "\n",
    "# Save and reload params.\n",
    "model_path = '/tmp/mjx_brax_quadruped_policy'\n",
    "model.save_params(model_path, params)\n",
    "params = model.load_params(model_path)\n",
    "\n",
    "inference_fn = make_inference_fn(params)\n",
    "jit_inference_fn = jax.jit(inference_fn)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
